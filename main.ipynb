{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8914496,"sourceType":"datasetVersion","datasetId":5360751},{"sourceId":190739193,"sourceType":"kernelVersion"}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Loader","metadata":{"_uuid":"95f18748-e5ac-4e5a-82ae-716748320d57","_cell_guid":"7f390aee-dbff-44b8-88ef-455f5c8ad8cc","trusted":true}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport pickle\nfrom tqdm.notebook import tqdm\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.model_selection import train_test_split, cross_val_predict, GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.metrics import make_scorer\nimport xgboost as xgb\nfrom scipy.optimize import curve_fit\nfrom scipy.fft import fft, fftfreq\nfrom scipy.stats import mode\nimport tensorflow as tf\nimport irp_utils\nimport time","metadata":{"_uuid":"0f5c5ecd-d65c-4b0f-9bff-d4547be02353","_cell_guid":"2f6b0d9a-855d-43b1-b4a2-cc8ff4ad18b8","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-01T19:48:43.119027Z","iopub.execute_input":"2024-08-01T19:48:43.119452Z","iopub.status.idle":"2024-08-01T19:49:02.922688Z","shell.execute_reply.started":"2024-08-01T19:48:43.119419Z","shell.execute_reply":"2024-08-01T19:49:02.921261Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-08-01 19:48:47.730248: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-08-01 19:48:47.730395: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-08-01 19:48:47.940950: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"!cat /proc/cpuinfo ","metadata":{"execution":{"iopub.status.busy":"2024-08-01T19:49:02.925265Z","iopub.execute_input":"2024-08-01T19:49:02.926185Z","iopub.status.idle":"2024-08-01T19:49:04.138646Z","shell.execute_reply.started":"2024-08-01T19:49:02.926137Z","shell.execute_reply":"2024-08-01T19:49:04.136435Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"processor\t: 0\nvendor_id\t: GenuineIntel\ncpu family\t: 6\nmodel\t\t: 79\nmodel name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\nstepping\t: 0\nmicrocode\t: 0xffffffff\ncpu MHz\t\t: 2199.998\ncache size\t: 56320 KB\nphysical id\t: 0\nsiblings\t: 4\ncore id\t\t: 0\ncpu cores\t: 2\napicid\t\t: 0\ninitial apicid\t: 0\nfpu\t\t: yes\nfpu_exception\t: yes\ncpuid level\t: 13\nwp\t\t: yes\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\nbugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa mmio_stale_data retbleed bhi\nbogomips\t: 4399.99\nclflush size\t: 64\ncache_alignment\t: 64\naddress sizes\t: 46 bits physical, 48 bits virtual\npower management:\n\nprocessor\t: 1\nvendor_id\t: GenuineIntel\ncpu family\t: 6\nmodel\t\t: 79\nmodel name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\nstepping\t: 0\nmicrocode\t: 0xffffffff\ncpu MHz\t\t: 2199.998\ncache size\t: 56320 KB\nphysical id\t: 0\nsiblings\t: 4\ncore id\t\t: 1\ncpu cores\t: 2\napicid\t\t: 2\ninitial apicid\t: 2\nfpu\t\t: yes\nfpu_exception\t: yes\ncpuid level\t: 13\nwp\t\t: yes\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\nbugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa mmio_stale_data retbleed bhi\nbogomips\t: 4399.99\nclflush size\t: 64\ncache_alignment\t: 64\naddress sizes\t: 46 bits physical, 48 bits virtual\npower management:\n\nprocessor\t: 2\nvendor_id\t: GenuineIntel\ncpu family\t: 6\nmodel\t\t: 79\nmodel name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\nstepping\t: 0\nmicrocode\t: 0xffffffff\ncpu MHz\t\t: 2199.998\ncache size\t: 56320 KB\nphysical id\t: 0\nsiblings\t: 4\ncore id\t\t: 0\ncpu cores\t: 2\napicid\t\t: 1\ninitial apicid\t: 1\nfpu\t\t: yes\nfpu_exception\t: yes\ncpuid level\t: 13\nwp\t\t: yes\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\nbugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa mmio_stale_data retbleed bhi\nbogomips\t: 4399.99\nclflush size\t: 64\ncache_alignment\t: 64\naddress sizes\t: 46 bits physical, 48 bits virtual\npower management:\n\nprocessor\t: 3\nvendor_id\t: GenuineIntel\ncpu family\t: 6\nmodel\t\t: 79\nmodel name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\nstepping\t: 0\nmicrocode\t: 0xffffffff\ncpu MHz\t\t: 2199.998\ncache size\t: 56320 KB\nphysical id\t: 0\nsiblings\t: 4\ncore id\t\t: 1\ncpu cores\t: 2\napicid\t\t: 3\ninitial apicid\t: 3\nfpu\t\t: yes\nfpu_exception\t: yes\ncpuid level\t: 13\nwp\t\t: yes\nflags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\nbugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa mmio_stale_data retbleed bhi\nbogomips\t: 4399.99\nclflush size\t: 64\ncache_alignment\t: 64\naddress sizes\t: 46 bits physical, 48 bits virtual\npower management:\n\n","output_type":"stream"}]},{"cell_type":"code","source":"## Install wandb\n!pip install wandb\n\n# Import necessary libraries\nimport wandb\nfrom kaggle_secrets import UserSecretsClient\n\n\n\nsecrets = UserSecretsClient()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-01T19:49:04.140830Z","iopub.execute_input":"2024-08-01T19:49:04.141576Z","iopub.status.idle":"2024-08-01T19:49:22.756575Z","shell.execute_reply.started":"2024-08-01T19:49:04.141520Z","shell.execute_reply":"2024-08-01T19:49:22.755229Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.17.4)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.41)\nRequirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from wandb) (3.11.0)\nRequirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0.1)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.32.3)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.8.0)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (69.0.3)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.7.4)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n","output_type":"stream"}]},{"cell_type":"code","source":"wandb_user = secrets.get_secret(\"wandb_user\")\nwandb_pass = secrets.get_secret(\"wandb-secret-key\")\nwandb.login()","metadata":{"execution":{"iopub.status.busy":"2024-08-01T19:49:22.759592Z","iopub.execute_input":"2024-08-01T19:49:22.760018Z","iopub.status.idle":"2024-08-01T20:13:43.956877Z","shell.execute_reply.started":"2024-08-01T19:49:22.759980Z","shell.execute_reply":"2024-08-01T20:13:43.950203Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\n\nKeyboardInterrupt\n\n","output_type":"stream"}]},{"cell_type":"code","source":"wandb.init(entity=wandb_user)\ninit_time = time.time()","metadata":{"execution":{"iopub.status.busy":"2024-08-01T20:13:43.957850Z","iopub.status.idle":"2024-08-01T20:13:43.958268Z","shell.execute_reply.started":"2024-08-01T20:13:43.958078Z","shell.execute_reply":"2024-08-01T20:13:43.958096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"time_table = wandb.Table(columns=[\"Event\",\"Time Since Initialization (seconds)\"])\n\ndef log_time(event_name):\n    elapsed_time = time.time() - init_time\n    time_table.add_data(event_name,elapsed_time)\n    wandb.log({event_name: elapsed_time})","metadata":{"execution":{"iopub.status.busy":"2024-08-01T20:13:43.960618Z","iopub.status.idle":"2024-08-01T20:13:43.961068Z","shell.execute_reply.started":"2024-08-01T20:13:43.960846Z","shell.execute_reply":"2024-08-01T20:13:43.960862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Choose Mission","metadata":{"_uuid":"9117680b-411f-48f7-8222-61e6aaa2f32c","_cell_guid":"b463e6a4-cb73-4a50-9fd3-4cb80579df38","trusted":true}},{"cell_type":"code","source":"mission_choice = 2\n\nchannels_to_load = [75,76,77,83,84,85]\n\n#1\n\n#[12,13,14] #1 Group 3, Doesnt Work - good score but different resolutions for anomalous sections\n#[12,13,19,20,27,28,36,37] # 1 Group 3, Doesnt Work\n#[27,28,36,37] 1 Group 3, works\n#[14,21,29] #1 Group 4, Works\n#[14,21,29,30,38] # 1 Group 4, Works\n#[17,18,25,26,34,35] # 1 Group 5, Doesnt Work - good score but different resolutions for anomalous sections\n#[16,24,32,33,40] # 1 Group 6, Works\n#[15,22,23,31,39] # 1 Group 7, Works\n#[41,42,43,44,45,46] # 1 Group 8, Works\n#[47,48,49] #1 Group 9, Works : 55\n#[50] 1 Group 10, Works\n#[50,51,52] # 1 Group 10, Works\n#[57,58,59,60] # 1 Group 13, Works (doesnt work with mem)\n#[61,62,63] # 1 Group 14 Requires slice with anomaly in to train\n#[64,65] # 1 Group 15, Works but requires larger slice of data\n\n#2\n\n\n#[9,10,11,12,13,14] Doesnt Work\n#[9,10,11,12,13] works\n#[25,26,27,28] # 2 Group 1, Doesnt work\n#[21,22,23,24] 2 Group 2,Works: 520\n#[18,19,20] # 2 Group 5,Works 680\n#[15,16,17] # 2 Group 8, Works\n#[73,81] # 2 Group 11, Need to find anomaly in the data slice\n#[75,76,77,83,84,85] # 2 Group 31, Need to find anomaly in the data slice","metadata":{"_uuid":"372f8b4c-d3d1-4055-9d75-e0e5049a8f04","_cell_guid":"387d57ee-6f82-4068-8591-e886d0cc9ad5","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-01T20:13:43.962207Z","iopub.status.idle":"2024-08-01T20:13:43.962603Z","shell.execute_reply.started":"2024-08-01T20:13:43.962416Z","shell.execute_reply":"2024-08-01T20:13:43.962432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load data","metadata":{"_uuid":"5d2097c8-cc95-4ced-bbd9-5bf70bf814b5","_cell_guid":"407bc410-8d04-4c6b-94f7-19767559b968","trusted":true}},{"cell_type":"code","source":"log_time(\"Dataset Loading Start\")\n\n# Initialize an empty DataFrame to store all channels' data\ndata = pd.DataFrame()\n\n# Load data for each channel and add to the DataFrame\nfor channel_choice in channels_to_load:\n    # File path\n    file_path = f'/kaggle/input/esa-anomaly-dataset/ESA-Mission{mission_choice}/ESA-Mission{mission_choice}/channels/channel_{channel_choice}/channel_{channel_choice}'\n\n    # Open the file and load it with pickle\n    with open(file_path, 'rb') as file:  # Note 'rb' mode for reading binary files\n        channel_data = pickle.load(file)\n    \n    # Rename the column to include channel number\n    channel_data.rename(columns={f'channel_{channel_choice}': f'channel_{channel_choice}'}, inplace=True)\n    \n    # Merge with the main data DataFrame\n    if data.empty:\n        data = channel_data\n    else:\n        data = data.merge(channel_data, left_index=True, right_index=True, how='outer')\n\n# Display the DataFrame\ndisplay(data.head())\nprint(f\"Length of merged data: {len(data)}\")","metadata":{"_uuid":"36381be6-0a3a-407d-816a-78d77184beab","_cell_guid":"8b7a71ee-1a16-421b-9c8c-f33eaec7aa79","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-01T20:13:43.964632Z","iopub.status.idle":"2024-08-01T20:13:43.965097Z","shell.execute_reply.started":"2024-08-01T20:13:43.964856Z","shell.execute_reply":"2024-08-01T20:13:43.964873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load Labels","metadata":{"_uuid":"af9c5ce3-8b59-4134-9493-402014d1a4f9","_cell_guid":"bfedcb08-1179-4e71-840d-e86b027c0933","trusted":true}},{"cell_type":"code","source":"# Load the labels data\nlabels = pd.read_csv(f'/kaggle/input/esa-anomaly-dataset/ESA-Mission{mission_choice}/ESA-Mission{mission_choice}/labels.csv')\n\n# Convert datetime columns correctly using .loc to avoid SettingWithCopyWarning\nlabels.loc[:, 'Start Time'] = pd.to_datetime(labels['StartTime'])\nlabels.loc[:, 'End Time'] = pd.to_datetime(labels['EndTime'])\n\n# Convert to datetime and remove timezone information to make them timezone-naive\nlabels['StartTime'] = pd.to_datetime(labels['StartTime']).dt.tz_localize(None)\nlabels['EndTime'] = pd.to_datetime(labels['EndTime']).dt.tz_localize(None)\n\n# Initialize the 'labels' column to 0 (no anomaly) if not already done\nif 'labels' not in data.columns:\n    data['labels'] = 0\n\n# Iterate through each channel and update the labels\nfor channel_choice in channels_to_load:\n    # Filter labels for the specific channel\n    anomalies = labels[labels['Channel'] == f'channel_{channel_choice}'].copy()\n    \n    # Update the labels for anomalies\n    for _, row in anomalies.iterrows():\n        # Get the boolean mask where the index is within the anomaly span\n        mask = (data.index >= row['StartTime']) & (data.index <= row['EndTime'])\n        \n        # Set the label to 1 for anomalies (without overwriting existing 1's)\n        data.loc[mask, 'labels'] = data.loc[mask, 'labels'].apply(lambda x: 1 if x == 0 else x)\n\n# Display the head of the dataset to verify labels\ndisplay(data.head())","metadata":{"_uuid":"85a544b4-8b04-4732-8456-e71317b33bcb","_cell_guid":"f0f7553d-6b0c-4283-bc3b-7d20becc58ce","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-01T20:13:43.966391Z","iopub.status.idle":"2024-08-01T20:13:43.966822Z","shell.execute_reply.started":"2024-08-01T20:13:43.966620Z","shell.execute_reply":"2024-08-01T20:13:43.966638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Plot Dataset","metadata":{"_uuid":"ad9f2f52-0c0b-4a21-96f8-bfeeeeec4042","_cell_guid":"06055ed2-aa5e-4f3b-9b1c-96e807672b0f","trusted":true}},{"cell_type":"code","source":"# # Plot the data and anomalies for each channel\n# plt.figure(figsize=(15, 7))\n# for channel_choice in channels_to_load:\n#     plt.plot(data.index, data[f'channel_{channel_choice}'], label=f'Channel {channel_choice}', linestyle='-')\n\n# # Plot anomalies\n# for _, row in anomalies.iterrows():\n#     plt.axvspan(row['Start Time'], row['End Time'], color='red', alpha=0.3, label='Anomaly')\n\n# # Handle legend labels (to avoid duplicate labels in the legend)\n# handles, labels = plt.gca().get_legend_handles_labels()\n# by_label = dict(zip(labels, handles))\n# plt.legend(by_label.values(), by_label.keys())\n\n# plt.title('Measurements Over Time with Anomalies for Multiple Channels')\n# plt.xlabel('Datetime')\n# plt.ylabel('Measurement Value')\n# plt.grid(True)\n# plt.xticks(rotation=45)\n# plt.tight_layout()\n# plt.show()","metadata":{"_uuid":"2803d596-5b98-4fcd-8acf-7922e4bbd5ba","_cell_guid":"5c9b085c-7b32-4c3e-86de-056fec53ba7f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-01T20:13:43.968745Z","iopub.status.idle":"2024-08-01T20:13:43.969350Z","shell.execute_reply.started":"2024-08-01T20:13:43.969074Z","shell.execute_reply":"2024-08-01T20:13:43.969098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"full_machine_data = data[[f'channel_{channel_choice}' for channel_choice in channels_to_load]].copy()  # Only data\nfull_machine_labels = data[['labels']].copy()    # Only labels\n# Verify that both DataFrames are correctly set up\nprint(full_machine_data.head())\nprint(full_machine_labels.head())\n\nfull_machine_data = full_machine_data.fillna(0)\n#full_machine_labels = full_machine_data.fillna(1)\n\nlog_time(\"Dataset Loading End\")","metadata":{"_uuid":"00fb5e7d-7015-4594-b666-092d4733f307","_cell_guid":"55e82405-22f8-4947-af53-622718721d34","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-01T20:13:43.971002Z","iopub.status.idle":"2024-08-01T20:13:43.971395Z","shell.execute_reply.started":"2024-08-01T20:13:43.971209Z","shell.execute_reply":"2024-08-01T20:13:43.971225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Sectioning Data","metadata":{"_uuid":"d36b91f9-dd45-46bf-a82f-4695c62b9ab3","_cell_guid":"13a4cb74-724d-4e22-9226-b8fcfba9debb","trusted":true}},{"cell_type":"code","source":"log_time(\"Sectioning Dataset Start\")\n# Calculate the index for the 20% slice\nslice_size = int(len(full_machine_data) * 0.08)\n\n\n# Take a contiguous slice of the data and labels\nmachine_data = full_machine_data.iloc[:slice_size]\nmachine_labels = full_machine_labels.iloc[:slice_size]\n                                            \nprint(\"Training Data Shape:\", machine_data.shape)\n\nlog_time(\"Sectioning Dataset End\")\n","metadata":{"_uuid":"862cc090-e316-4613-af39-f83c4fa890bf","_cell_guid":"5b340ee9-3d21-4b08-bf3b-adb90d8293e8","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-01T20:13:43.972811Z","iopub.status.idle":"2024-08-01T20:13:43.973231Z","shell.execute_reply.started":"2024-08-01T20:13:43.973039Z","shell.execute_reply":"2024-08-01T20:13:43.973056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the data and anomalies for each channel\nplt.figure(figsize=(15, 7))\n\n# Plot anomalies\nanomaly_indices = machine_labels[machine_labels['labels'] == 1].index\nfor anomaly_index in anomaly_indices:\n    plt.axvline(x=anomaly_index, color='red', alpha=0.1, label='Anomaly')\n\nfor channel_choice in channels_to_load:\n    plt.plot(machine_data.index, machine_data[f'channel_{channel_choice}'], label=f'Channel {channel_choice}', linestyle='-')\n    \n# Handle legend labels (to avoid duplicate labels in the legend)\nhandles, labels = plt.gca().get_legend_handles_labels()\nby_label = dict(zip(labels, handles))\nplt.legend(by_label.values(), by_label.keys())\n\nplt.title('Measurements Over Time with Anomalies for Multiple Channels')\nplt.xlabel('Datetime')\nplt.ylabel('Measurement Value')\nplt.grid(True)\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()","metadata":{"_uuid":"00d7d2fd-96e3-4187-927f-59d3b4f379bd","_cell_guid":"edfc046a-2ad2-46f6-8950-55a30b48d5a4","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-01T20:13:43.974695Z","iopub.status.idle":"2024-08-01T20:13:43.975155Z","shell.execute_reply.started":"2024-08-01T20:13:43.974922Z","shell.execute_reply":"2024-08-01T20:13:43.974939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Get Sub-Cycles","metadata":{"_uuid":"a13036e9-e25e-450f-ae18-4562971a3474","_cell_guid":"46619a47-f735-4ba7-a043-2d274e350e77","trusted":true}},{"cell_type":"code","source":"sub_cycle_divisor = 6\nmax_cycle_length = 500\nmin_cycle_length = 100\n\nuser_defined_cycle_length = False\nuse_average_cycle_length = False\nuse_most_common_cycle_length = True\nuse_individual_cycle_length = False","metadata":{"_uuid":"5ffcf026-a910-4ff5-bb34-84dcede0837b","_cell_guid":"64e6d091-c31b-4c5d-aeb8-9b1de52959df","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-01T20:13:43.976721Z","iopub.status.idle":"2024-08-01T20:13:43.977164Z","shell.execute_reply.started":"2024-08-01T20:13:43.976941Z","shell.execute_reply":"2024-08-01T20:13:43.976975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"log_time(\"Getting Sub-Cycles Start\")\n\n# Initialize the dictionary to store sub-cycle outputs\nsub_cycles = {}\ncycle_lengths = []\n\nirp_utils.validate_cycle_length_selection(user_defined_cycle_length, use_average_cycle_length, use_most_common_cycle_length, use_individual_cycle_length)\n\nif not user_defined_cycle_length:\n    # Loop through all features and store the sub-cycle outputs\n    for feature in range(machine_data.shape[1]):\n        nominal_sub_cycles, anomalous_sub_cycles, cycle_length = irp_utils.get_subcycle(feature, sub_cycle_divisor, machine_data, \n                                                                              machine_labels, max_cycle_length=max_cycle_length,\n                                                                              min_cycle_length=min_cycle_length,plot_results=False)\n\n        if cycle_length is not None:\n            cycle_lengths.append(cycle_length)\n\n        # Initialize the sub-cycles for the current feature\n        sub_cycles[feature] = {\n            'nominal': [],\n            'anomalous': []\n        }\n\n        if not user_defined_cycle_length and not use_average_cycle_length and not use_most_common_cycle_length:\n            # Append the nominal and anomalous sub-cycles for the current feature\n            sub_cycles[feature]['nominal'].append(nominal_sub_cycles)\n            sub_cycles[feature]['anomalous'].append(anomalous_sub_cycles)\n\n        \n\n# Filter out None values from cycle_lengths\ncycle_lengths = [cl for cl in cycle_lengths if cl is not None]\n\nif user_defined_cycle_length or use_average_cycle_length or use_most_common_cycle_length:\n\n\n    if user_defined_cycle_length:\n        final_cycle_length = user_defined_cycle_length\n        \n    elif use_average_cycle_length:\n        final_cycle_length = int(np.mean(cycle_lengths))\n        \n    elif use_most_common_cycle_length:\n        counts = np.bincount(cycle_lengths)\n        final_cycle_length = np.argmax(counts)\n        print(f\"FINAL:{final_cycle_length}\")\n        \n    elif use_individual_cycle_length:\n        final_cycle_length = None\n    else:\n        final_cycle_length = max_cycle_length\n\n    print(f\"Final Cycle Length: {final_cycle_length}\")\n    \n    # Apply the final cycle length to all features\n    for feature in tqdm(range(machine_data.shape[1])):\n        \n        # Initialize the sub-cycles for the current feature\n        sub_cycles[feature] = {\n            'nominal': [],\n            'anomalous': []\n        }\n        \n        nominal_sub_cycles, anomalous_sub_cycles, _ = irp_utils.get_subcycle(feature, sub_cycle_divisor, machine_data, \n                                                                   machine_labels, max_cycle_length=max_cycle_length, \n                                                                   min_cycle_length=min_cycle_length,\n                                                                   plot_results=False, user_cycle_length=final_cycle_length,\n                                                                   use_individual_cycle_lengths=use_individual_cycle_length)\n\n        sub_cycles[feature]['nominal'].append(nominal_sub_cycles)\n        sub_cycles[feature]['anomalous'].append(anomalous_sub_cycles)\n        \nlog_time(\"Getting Sub-Cycles End\")\n","metadata":{"_uuid":"fa2bfc78-d14c-4629-852b-0495d1deb2a6","_cell_guid":"7018e247-590c-4cd1-8107-3ded37b15c6d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-01T20:13:43.978815Z","iopub.status.idle":"2024-08-01T20:13:43.979244Z","shell.execute_reply.started":"2024-08-01T20:13:43.979041Z","shell.execute_reply":"2024-08-01T20:13:43.979060Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Cycle Lengths: {cycle_lengths}\")\n\nsub_cycle_lengths = [int(x / sub_cycle_divisor) for x in cycle_lengths]\nprint(f\"Sub-Cycle Lengths: {sub_cycle_lengths}\")","metadata":{"_uuid":"377b3ec7-7934-46b6-aae5-417147e234c0","_cell_guid":"95f4b010-0ebf-4c73-a87e-af4ea7b13577","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-01T20:13:43.980488Z","iopub.status.idle":"2024-08-01T20:13:43.980862Z","shell.execute_reply.started":"2024-08-01T20:13:43.980674Z","shell.execute_reply":"2024-08-01T20:13:43.980689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assuming sub_cycles is a dictionary with the required data\nnum_features = len(sub_cycles)\n\nfor feature in range(num_features):\n    fig, axes = plt.subplots(2, sub_cycle_divisor, figsize=(18, 12), sharex=True)\n    \n    for i in range(sub_cycle_divisor):  # There are sub-cycles for nominal and anomalous\n        # Plot nominal sub-cycles for the current feature\n        for cycle in sub_cycles[feature]['nominal'][0][i]:\n            if isinstance(cycle, (list, np.ndarray)):\n                axes[0, i].plot(np.arange(len(cycle)), cycle)\n        axes[0, i].set_title(f'Nominal SC {i+1} for Ch {feature}')\n        axes[0, i].set_ylabel('Normalized Value')\n        axes[0, i].axhline(y=0, color='black', linestyle='--', linewidth=1)\n        \n        # Plot anomalous sub-cycles for the current feature\n        for cycle in sub_cycles[feature]['anomalous'][0][i]:\n            if isinstance(cycle, (list, np.ndarray)):\n                axes[1, i].plot(np.arange(len(cycle)), cycle)\n        axes[1, i].set_title(f'Anomalous SC {i+1} for Ch {feature}')\n        axes[1, i].set_ylabel('Normalized Value')\n        axes[1, i].axhline(y=0, color='black', linestyle='--', linewidth=1)\n    \n    for ax in axes[1, :]:\n        ax.set_xlabel('Time')\n    \n    plt.tight_layout()\n    plt.show()","metadata":{"_uuid":"a1af1678-5f83-4de9-bc5e-2dea3710510d","_cell_guid":"3badd2b5-30fd-4fc5-84b5-7485e57c0f42","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-01T20:13:43.982775Z","iopub.status.idle":"2024-08-01T20:13:43.983217Z","shell.execute_reply.started":"2024-08-01T20:13:43.983016Z","shell.execute_reply":"2024-08-01T20:13:43.983034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"log_time(\"Getting Features Start\")\n\n# Calculate similarities\nnom_comparisons = 90\nanom_comparisons = 30\nnominal_similarities, anomalous_similarities = irp_utils.calculate_similarity(sub_cycles,nom_comparisons,anom_comparisons,sub_cycle_divisor)\n\n# Convert to DataFrames\nanomalous_df = pd.DataFrame(anomalous_similarities)\nnominal_df = pd.DataFrame(nominal_similarities)\n\nlog_time(\"Getting Features End\")\n","metadata":{"_uuid":"fedbd92f-366e-410b-a3a4-188d706dc4be","_cell_guid":"67cb7c34-f1e1-4c99-93d0-6b289414b5c6","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-01T20:13:43.985627Z","iopub.status.idle":"2024-08-01T20:13:43.986245Z","shell.execute_reply.started":"2024-08-01T20:13:43.985909Z","shell.execute_reply":"2024-08-01T20:13:43.985932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display DataFrames\nprint(\"Anomalous Sub-cycle Similarities:\")\nanomalous_df = anomalous_df.fillna(0)\nanomalous_df","metadata":{"_uuid":"24171ba8-9b75-4905-88d8-5a690fe74572","_cell_guid":"c4bc6af7-d295-4b2a-8cf1-9b1a16e98ce3","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-01T20:13:43.988234Z","iopub.status.idle":"2024-08-01T20:13:43.988829Z","shell.execute_reply.started":"2024-08-01T20:13:43.988501Z","shell.execute_reply":"2024-08-01T20:13:43.988525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Nominal Sub-cycle Similarities:\")\nnominal_df = nominal_df.fillna(0)\nnominal_df","metadata":{"_uuid":"a8b5315b-2c5a-41d0-b3d9-73bbd29a94bb","_cell_guid":"8a5cf1f2-9830-471e-827c-58e352a9f81e","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-01T20:13:43.990508Z","iopub.status.idle":"2024-08-01T20:13:43.991113Z","shell.execute_reply.started":"2024-08-01T20:13:43.990813Z","shell.execute_reply":"2024-08-01T20:13:43.990837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"anomalous_data_loaded = anomalous_df.drop(columns=['comparison_type'])#,'rolling_mean_diff','rolling_std_diff','fft_mean_diff','fft_std_diff','fft_max_diff'])\nnominal_data_loaded = nominal_df.drop(columns=['comparison_type'])#,'rolling_mean_diff','rolling_std_diff','fft_mean_diff','fft_std_diff','fft_max_diff'])\n\n# anomalous_data_loaded = anomalous_data_loaded.drop(columns=['euclidean_distance','rmse', 'std_diff', 'min_diff', 'max_diff', 'mean_diff', 'fft_diff','var_diff',\n#             'mad_diff','iqr_diff','mad_mean_diff','energy_diff','skew_diff','kurtosis_diff','spectral_entropy_diff'])#'rolling_mean_diff','rolling_std_diff','fft_mean_diff','fft_std_diff','fft_max_diff'])\n# nominal_data_loaded = nominal_data_loaded.drop(columns=['euclidean_distance','rmse', 'std_diff', 'min_diff', 'max_diff', 'mean_diff', 'fft_diff','var_diff',\n#             'mad_diff','iqr_diff','mad_mean_diff','energy_diff','skew_diff','kurtosis_diff','spectral_entropy_diff'])#'rolling_mean_diff','rolling_std_diff','fft_mean_diff','fft_std_diff','fft_max_diff'])","metadata":{"_uuid":"e1fa8eba-49c8-4f2a-b00a-15a5ae445c7b","_cell_guid":"051d35b5-066c-4ec3-95ed-da9a8b98d82a","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-01T20:13:43.992911Z","iopub.status.idle":"2024-08-01T20:13:43.993461Z","shell.execute_reply.started":"2024-08-01T20:13:43.993189Z","shell.execute_reply":"2024-08-01T20:13:43.993212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{"_uuid":"829c9adb-2de3-4532-986c-5eb1c10d15b3","_cell_guid":"af7bc556-d6a3-4067-a976-8db7b2019649","trusted":true}},{"cell_type":"code","source":"# Prediction imports\nimport os\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense, Dropout,BatchNormalization,Lambda,InputLayer, GaussianNoise, add\nfrom tensorflow.keras.losses import MeanSquaredError\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.regularizers import l1, l2, l1_l2\nfrom tqdm.notebook import tqdm\nimport re\n\nfrom tensorflow.keras.callbacks import EarlyStopping\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay,f1_score, make_scorer,precision_recall_fscore_support\nfrom sklearn.impute import SimpleImputer\nimport time\nimport random\nimport gc\nimport json\nimport ipywidgets as widgets\nfrom IPython.display import display, clear_output","metadata":{"_uuid":"dfb3314c-b8fc-4862-8fb6-9cce1057412a","_cell_guid":"94366ca9-eca7-4a92-bf78-bbd3f632c0d6","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-01T20:13:43.995262Z","iopub.status.idle":"2024-08-01T20:13:43.995637Z","shell.execute_reply.started":"2024-08-01T20:13:43.995449Z","shell.execute_reply":"2024-08-01T20:13:43.995465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nominal_df = nominal_data_loaded\nanomalous_df = anomalous_data_loaded","metadata":{"_uuid":"391fc11e-161b-49c1-9d79-0bbef0ca6638","_cell_guid":"a572fe98-d3c4-4c10-baa0-bfbdd6bcc915","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-01T20:13:43.998006Z","iopub.status.idle":"2024-08-01T20:13:43.998408Z","shell.execute_reply.started":"2024-08-01T20:13:43.998214Z","shell.execute_reply":"2024-08-01T20:13:43.998229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Ensure TensorFlow uses the GPU\nphysical_devices = tf.config.list_physical_devices('GPU')\nif len(physical_devices) > 0:\n    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n    \ngc.collect()","metadata":{"_uuid":"07b4eb5e-9264-4bc5-b932-cbec1dac5b5f","_cell_guid":"8d426507-cd5a-4c3f-92eb-1a1609a80097","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-01T20:13:44.000396Z","iopub.status.idle":"2024-08-01T20:13:44.000846Z","shell.execute_reply.started":"2024-08-01T20:13:44.000640Z","shell.execute_reply":"2024-08-01T20:13:44.000658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# List of features for training\nfeatures = ['pearson_correlation', 'spearman_correlation', 'euclidean_distance', 'cosine_similarity', \n            'rmse', 'std_diff', 'min_diff', 'max_diff', 'mean_diff', 'fft_diff','var_diff',\n            'mad_diff','iqr_diff','mad_mean_diff','energy_diff','skew_diff','kurtosis_diff','spectral_entropy_diff','rolling_mean_diff','rolling_std_diff','fft_mean_diff','fft_std_diff','fft_max_diff']#,'dtw_distance']","metadata":{"_uuid":"32a36d53-6548-4f42-b2cd-ee1cade0a92b","_cell_guid":"2b8751e0-b810-4130-bc78-9ec720fbcce7","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-01T20:13:44.002309Z","iopub.status.idle":"2024-08-01T20:13:44.002693Z","shell.execute_reply.started":"2024-08-01T20:13:44.002507Z","shell.execute_reply":"2024-08-01T20:13:44.002523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Separate the data by channels and split before any processing\nunique_channels = nominal_df['feature'].unique()\nsplit_data = {}\n\n# Split the data for each channel\nprint(\"Splitting data by channels\")\nfor channel in tqdm(unique_channels):\n    #print(f\"Channel {channel}\")\n    \n    # Filter the data for the current feature\n    nominal_data = nominal_df[nominal_df['feature'] == channel]\n    anomalous_data = anomalous_df[anomalous_df['feature'] == channel]\n\n    # Drop unnecessary columns\n    #nominal_data = nominal_data.drop(columns=['nominal_cycle_1', 'nominal_cycle_2'])#,'target']) #, 'feature'])\n    #anomalous_data = anomalous_data.drop(columns=['target'])\n    \n    # Split the nominal data into training, validation, and test sets\n    X_train, X_val, X_test_nominal = irp_utils.split_nominal_data(nominal_data)\n\n    # Store the split data\n    split_data[channel] = {\n        'X_train': X_train,\n        'X_val': X_val,\n        'X_test_nominal': X_test_nominal,\n        #'X_threshold': X_threshold,\n        'anomalous_data': anomalous_data\n    }","metadata":{"_uuid":"30ce425e-b95d-4a2a-8d61-9320add0a57e","_cell_guid":"f017126f-a336-44a2-bff2-9622a4368820","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-01T20:13:44.004087Z","iopub.status.idle":"2024-08-01T20:13:44.004482Z","shell.execute_reply.started":"2024-08-01T20:13:44.004293Z","shell.execute_reply":"2024-08-01T20:13:44.004310Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nlog_time(\"Training Start\")\n\n# Train autoencoders and prepare test data\nautoencoders = []\nval_data_list = []\nX_test_combined = []\nhistories = []\nscalers = []\n\nautoencoders_dict = {}\n\nprint(\"Creating Autoencoders\")\n\n# Record start time\nstart_time = time.time()\n\nfor channel in tqdm(unique_channels):\n\n    #print(f\"Channel {channel}\")\n    \n    data = split_data[channel]\n    \n    # Normalize the data\n    #scaler = StandardScaler()\n    scaler = MinMaxScaler()\n    \n    X_train_scaled = scaler.fit_transform(data['X_train'])\n    X_val_scaled = scaler.transform(data['X_val'])\n    X_test_nominal_scaled = scaler.transform(data['X_test_nominal'])\n    anomalous_data_scaled = scaler.transform(data['anomalous_data'])\n    \n    #Put scaler in the list\n    scalers.append(scaler)\n    \n    # Train autoencoder for the feature\n\n    autoencoder, X_val, history = irp_utils.train_autoencoder(X_train_scaled,X_val_scaled,autoencoders_dict, channel)\n\n    autoencoders.append(autoencoder)\n    val_data_list.append(X_val_scaled)  # Use X_val_scaled to ensure consistency\n    #threshold_data_list.append(X_threshold_scaled)\n    histories.append(history)\n    \n    # Prepare test data for the feature\n    X_test = np.concatenate([X_test_nominal_scaled, anomalous_data_scaled])\n    X_test_combined.append(X_test)\n\n# Record end time and calculate duration\nend_time = time.time()\ntraining_duration = end_time - start_time\nprint(f\"Training time: {training_duration}\")\n\n# Combine the test labels for nominal and anomalous sets\ny_test = np.concatenate([np.zeros(data['X_test_nominal'].shape[0]), np.ones(data['anomalous_data'].shape[0])])\n\n# Shuffle the test set\ntest_indices = np.arange(len(y_test))\nnp.random.shuffle(test_indices)\nX_test_combined = [X_test[test_indices] for X_test in X_test_combined]\ny_test = y_test[test_indices]\n\nlog_time(\"Training End\")","metadata":{"_uuid":"63c46f31-f384-4ef6-93ce-c374496269ef","_cell_guid":"5653cd8b-4ac3-4853-a3e0-78f7842dee3c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-01T20:13:44.006260Z","iopub.status.idle":"2024-08-01T20:13:44.006633Z","shell.execute_reply.started":"2024-08-01T20:13:44.006447Z","shell.execute_reply":"2024-08-01T20:13:44.006463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation","metadata":{"_uuid":"0d79b6ff-5d8a-4980-9ec9-65e85a842de8","_cell_guid":"3ec5a3ce-82bc-4e43-ae18-e266e25d55a1","trusted":true}},{"cell_type":"code","source":"log_time(\"Evaluation Start\")\n\nprint(\"Evaluating Autoencoders\")\n\n# Create a figure for the subplots\nfig, axes = plt.subplots(nrows=1, ncols=6, figsize=(30, 15))  # Adjust the size as necessary\naxes = axes.flatten()\n\n# Loop through thresholds from 50 to 100\nfor i, percentile_threshold in enumerate(range(50, 101,10)):\n    # Record start time\n    start_time = time.time()\n    \n    #results_df, f1c, f1, precision, recall, f1pa, threshold\n    # Evaluate the ensemble of autoencoders\n    results_df, f1c, f1, precision, recall, corrected_f1,channel_f1,alarming_prec,adtqc_score,affiliation_f1, threshold= irp_utils.evaluate_ensemble(\n        autoencoders, X_test_combined, y_test, val_data_list, percentile_threshold\n    )\n\n    # Generate the confusion matrix\n    cm = confusion_matrix(results_df['Actual Label'], results_df['Predicted Anomaly'])\n    \n    # Plot the confusion matrix\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Nominal', 'Anomalous'])\n    disp.plot(ax=axes[i], colorbar=False)\n    axes[i].set_title(f\"Threshold: {percentile_threshold}\")\n\n    # Record end time and calculate duration\n    end_time = time.time()\n    evaluation_duration = end_time - start_time\n\n    # Print results (optional)\n    print(f\"Threshold: {percentile_threshold}\")\n    print(f\"Evaluation time: {evaluation_duration}\")\n    print(f\"F1C: {f1c}, F1: {f1}, Precision: {precision}, Recall: {recall}, Threshold used: {percentile_threshold}\")\n    print(\"=\"*50)\n\n    \nlog_time(\"Evaluation End\")\n# Adjust layout\nplt.tight_layout()\nplt.show()","metadata":{"_uuid":"9a057949-7f37-41ab-8004-d00fd663a1b9","_cell_guid":"01ccb64b-5cbb-44bc-ba00-ecd9ddc3c15c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-01T20:13:44.008079Z","iopub.status.idle":"2024-08-01T20:13:44.008444Z","shell.execute_reply.started":"2024-08-01T20:13:44.008264Z","shell.execute_reply":"2024-08-01T20:13:44.008280Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Evaluating Autoencoders\")\n# Record start time\nstart_time = time.time()\npercentile_threshold = 100\n# Evaluate the ensemble of autoencoders\nresults_df, f1c, f1, precision, recall, corrected_f1,channel_f1,alarming_prec,adtqc_score,affiliation_f1, threshold = irp_utils.evaluate_ensemble(autoencoders, X_test_combined, \n                                                                                      y_test, val_data_list,\n                                                                                      percentile_threshold)\n# Record end time and calculate duration\nend_time = time.time()\nevaluation_duration = end_time - start_time\nprint(f\"Evaluation time: {evaluation_duration}\")","metadata":{"_uuid":"e1e120da-5960-4ac7-8374-1b8ba257c775","_cell_guid":"1efb8f3c-2626-4df7-ad0a-5ed03070f707","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-01T20:13:44.010119Z","iopub.status.idle":"2024-08-01T20:13:44.010483Z","shell.execute_reply.started":"2024-08-01T20:13:44.010303Z","shell.execute_reply":"2024-08-01T20:13:44.010318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the results and confusion matrix\nplt.figure(figsize=(14, 7))\n\n# Plot reconstruction error\nplt.subplot(4, 1, 1)\nplt.plot(results_df['Reconstruction Error'], label='Reconstruction Error')\nplt.axhline(y=threshold, color='r', linestyle='--', label='Threshold')\nplt.title(f'Reconstruction Error and Threshold')\nplt.legend()\n\n# # Plot actual vs predicted\n# plt.subplot(4, 1, 2)\n# plt.plot(results_df['Actual Label'], label='Actual Label', alpha=0.7)\n# plt.plot(results_df['Predicted Anomaly'], label='Predicted Anomaly', alpha=0.3)\n# plt.title(f'Actual Labels vs Predicted Anomalies')\n# plt.legend()\n\n# Plot actual vs predicted highlighting correct and incorrect predictions\nplt.subplot(4, 1, 2)\ncorrect_predictions = results_df[results_df['Correct Prediction'] == True]\nincorrect_predictions = results_df[results_df['Correct Prediction'] == False]\n\n#plt.plot(correct_predictions.index, correct_predictions['Actual Label'], 'go', markersize=4, label='Correct Predictions')\n#plt.plot(incorrect_predictions.index, incorrect_predictions['Actual Label'], 'ro', markersize=4, label='Incorrect Predictions')\noffset = 0.2\nplt.plot(correct_predictions.index, correct_predictions['Actual Label'], 'go', markersize=4, label='Correct Predictions')\nplt.plot(incorrect_predictions.index, incorrect_predictions['Actual Label'] + offset, 'ro', markersize=4, label='Incorrect Predictions')\n\nplt.title(f'Actual Labels vs Predicted Anomalies (Correct and Incorrect Predictions)')\nplt.legend()\n\n# Plot confusion matrix\nplt.subplot(4, 1, 3)\ncm = confusion_matrix(results_df['Actual Label'], results_df['Predicted Anomaly'])\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Nominal', 'Anomalous'])\ndisp.plot(ax=plt.gca())\nplt.title(f'Confusion Matrix')\n\n\nplt.tight_layout()\nplt.show()\n\nprint(f'f1c Score: {f1c}')\nprint(f'f1 Score: {f1}')\nprint(f'Precision: {precision}')\nprint(f'Recall: {recall}')\n#print(f\"f1pa Score: {f1pa}\")\nprint(f'f1_corrected: {corrected_f1}')\nprint(f'Channel Aware f1: {channel_f1}')\nprint(f'Event Wise Alarming Precision: {alarming_prec}')\nprint(f'ADTQC: {adtqc_score}')\n#print(f'Affiliation-Based f1: {affiliation_f1}')\n\n# Plot the training stats\nplt.figure(figsize=(14, 7))\nfor i, history in enumerate(histories):\n    plt.plot(history.history['loss'], label=f'Channel {unique_channels[i]} - Training Loss')\nplt.title('Training Loss for All Channels')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), ncol=4)\nplt.show()\n\n# Plot the training stats\nplt.figure(figsize=(14, 7))\nfor i, history in enumerate(histories):\n    plt.plot(history.history['val_loss'], label=f'Channel {unique_channels[i]} - Validation Loss')\nplt.title('Validation Loss for All Channels')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), ncol=4)\nplt.show()\n\nresults_df","metadata":{"_uuid":"c38c1363-7099-4db9-9ff7-03cbea1d68d7","_cell_guid":"a681e23e-3a88-49a7-81e5-976d63f2d33c","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-01T20:13:44.012145Z","iopub.status.idle":"2024-08-01T20:13:44.012548Z","shell.execute_reply.started":"2024-08-01T20:13:44.012327Z","shell.execute_reply":"2024-08-01T20:13:44.012342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"autoencoders[0].summary()","metadata":{"execution":{"iopub.status.busy":"2024-08-01T20:13:44.013936Z","iopub.status.idle":"2024-08-01T20:13:44.014376Z","shell.execute_reply.started":"2024-08-01T20:13:44.014174Z","shell.execute_reply":"2024-08-01T20:13:44.014191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.utils import plot_model\n\n# Plot a model and save to a file\nplot_model(autoencoders[0], to_file='autoencoder_model_plot.png', show_shapes=True, show_layer_names=True)\n\n# Display the image\nfrom IPython.display import Image\nImage('autoencoder_model_plot.png')","metadata":{"execution":{"iopub.status.busy":"2024-08-01T20:13:44.015826Z","iopub.status.idle":"2024-08-01T20:13:44.016222Z","shell.execute_reply.started":"2024-08-01T20:13:44.016035Z","shell.execute_reply":"2024-08-01T20:13:44.016052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Testing on Live Data","metadata":{"_uuid":"1562d37a-fad7-4819-b6e8-45a3cb0a72b5","_cell_guid":"5fcf4abe-caa5-4ea2-8613-e7dd671b65ee","trusted":true}},{"cell_type":"code","source":"\ndef get_subcycle_with_length(feature, sub_cycle_length, machine_data, \n                             max_freq=1.0, initial_step=0.01, refinement_steps=3, \n                             max_cycle_length=1500, min_cycle_length=400, plot_results=True):\n    \"\"\"\n    Function to sort subcycles into arrays using a provided sub-cycle length.\n    Input: Chosen feature channel (int), Sub-cycle length (int), Data.\n    Output: Subcycles.\n    \"\"\"\n    print(f\"Feature {feature}\")\n\n    def estimate_cycle_length(y_data, step, already_checked_freqs):\n        offset_estimate = y_data.mean()\n        yf = fft(y_data - offset_estimate)\n        xf = fftfreq(len(y_data), 1)\n        half_len = len(yf) // 2\n        yf = yf[:half_len]\n        xf = xf[:half_len]\n        valid_range = xf <= max_freq\n        yf = yf[valid_range]\n        xf = xf[valid_range]\n        mask = np.isin(xf, already_checked_freqs, invert=True)\n        yf = yf[mask]\n        xf = xf[mask]\n        if len(xf) == 0:\n            return None, None\n        coarse_index = np.arange(0, len(xf), max(1, int(step)))\n        dominant_freq_index = coarse_index[np.argmax(2.0 / len(y_data) * np.abs(yf[coarse_index]))]\n        return xf[dominant_freq_index], dominant_freq_index\n\n    def find_cycle_length(y_data):\n        already_checked_freqs = []\n        dominant_freq, _ = estimate_cycle_length(y_data, initial_step, already_checked_freqs)\n        if dominant_freq:\n            already_checked_freqs.append(dominant_freq)\n        \n        for _ in range(refinement_steps):\n            initial_step = max(1, initial_step // 2)\n            new_freq, _ = estimate_cycle_length(y_data, initial_step, already_checked_freqs)\n            if new_freq:\n                dominant_freq = new_freq\n                already_checked_freqs.append(dominant_freq)\n\n        cycle_length = int(1 / dominant_freq) if dominant_freq else max_cycle_length\n\n        while (cycle_length < min_cycle_length or cycle_length > max_cycle_length) and dominant_freq:\n            if cycle_length < min_cycle_length:\n                new_freq, _ = estimate_cycle_length(y_data, initial_step, already_checked_freqs)\n                if new_freq:\n                    dominant_freq = new_freq\n                    cycle_length = int(1 / dominant_freq)\n                    already_checked_freqs.append(dominant_freq)\n            elif cycle_length > max_cycle_length:\n                new_freq, _ = estimate_cycle_length(y_data, initial_step, already_checked_freqs)\n                if new_freq:\n                    dominant_freq = new_freq\n                    cycle_length = int(1 / dominant_freq)\n                    already_checked_freqs.append(dominant_freq)\n            else:\n                break\n        \n        return cycle_length\n\n    x_data = np.arange(len(machine_data))\n    y_data = machine_data.iloc[:, feature].to_numpy()\n\n    # Use the provided sub-cycle length directly\n    cycle_length = sub_cycle_length\n\n    print(f\"Cycle Length: {cycle_length}\")\n\n    if plot_results and cycle_length >= min_cycle_length and cycle_length <= max_cycle_length:\n        plt.figure(figsize=(10, 6))\n        plt.plot(x_data, y_data, label='Data')\n        already_checked_freqs = []\n        dominant_freq, _ = estimate_cycle_length(y_data, initial_step, already_checked_freqs)\n        if dominant_freq:\n            already_checked_freqs.append(dominant_freq)\n            initial_guess = [(y_data.max() - y_data.min()) / 2, 2 * np.pi * dominant_freq, 0, y_data.mean()]\n            try:\n                params, _ = curve_fit(sine_wave, x_data, y_data, p0=initial_guess, maxfev=2000)\n                plt.plot(x_data, sine_wave(x_data, *params), color='lime', label='Fitted Sine Wave')\n            except RuntimeError as e:\n                print(f\"Curve fitting failed: {e}\")\n        for i in np.arange(0, len(x_data), cycle_length):\n            plt.axvline(x=i, color='black', linestyle='-', linewidth=1.5)\n        for i in np.arange(0, len(x_data), cycle_length):\n            plt.axvline(x=i, color='black', linestyle='--', linewidth=1)\n        plt.title(f'Sine Wave Fitting to Feature {feature}')\n        plt.xlabel('Time')\n        plt.ylabel('Normalized Value')\n        plt.legend()\n        plt.show()\n\n    sub_cycles = []\n\n    for start in range(0, len(x_data), cycle_length):\n        sub_cycle_end = start + cycle_length\n        if sub_cycle_end <= len(x_data):\n            sub_cycle_data = y_data[start:sub_cycle_end]\n            sub_cycles.append((start,sub_cycle_data))\n\n    return sub_cycles, cycle_length\n\n# Define the sine wave function used for curve fitting\ndef sine_wave(x, A, B, C, D):\n    return A * np.sin(B * x + C) + D","metadata":{"_uuid":"9e95d9a8-967c-41fa-90ac-24b643ba6b2b","_cell_guid":"7148e72f-0c3f-46ae-b2ae-462b241918a3","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-01T20:13:44.017351Z","iopub.status.idle":"2024-08-01T20:13:44.017751Z","shell.execute_reply.started":"2024-08-01T20:13:44.017560Z","shell.execute_reply":"2024-08-01T20:13:44.017577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_index = 100\nfinal_sub_cycle_length = int(final_cycle_length/sub_cycle_divisor)\n\nstart_slice = slice_size + final_sub_cycle_length*start_index\nend_slice = start_slice + final_sub_cycle_length*70\n# Take a new slice which will be used to simulate live data\nlive_data = full_machine_data.iloc[start_slice:end_slice]\nlive_labels = full_machine_labels.iloc[start_slice:end_slice]\n\nprint(\"Live Data Shape:\", live_data.shape)\nprint(live_data)","metadata":{"_uuid":"fd93e815-0a4b-4059-ac19-9320990d2eda","_cell_guid":"96ec6133-9170-41ec-a52d-30759acfb95a","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-01T20:13:44.019289Z","iopub.status.idle":"2024-08-01T20:13:44.019664Z","shell.execute_reply.started":"2024-08-01T20:13:44.019482Z","shell.execute_reply":"2024-08-01T20:13:44.019499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the data and anomalies for each channel\nplt.figure(figsize=(15, 7))\n\nsub_cycle_plot_length = final_sub_cycle_length\n# Add vertical blue lines every final_sub_cycle_length\nfor j in range(0, len(live_data),final_sub_cycle_length):\n    plt.axvline(x=live_data.index[j], color='blue', linestyle='--', alpha=0.7, label='Sub Cycle')\n\n\n# Plot anomalies\nanomaly_indices = live_labels[live_labels['labels'] == 1].index\nfor anomaly_index in anomaly_indices:\n    plt.axvline(x=anomaly_index, color='red', alpha=0.1, label='Anomaly')\n\nfor channel_choice in channels_to_load:\n    plt.plot(live_data.index, live_data[f'channel_{channel_choice}'], label=f'Channel {channel_choice}', linestyle='-')\n    \n# Handle legend labels (to avoid duplicate labels in the legend)\nhandles, labels = plt.gca().get_legend_handles_labels()\nby_label = dict(zip(labels, handles))\nplt.legend(by_label.values(), by_label.keys())\n\nplt.title('Measurements Over Time with Anomalies for Multiple Channels')\nplt.xlabel('Datetime')\nplt.ylabel('Measurement Value')\nplt.grid(True)\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()","metadata":{"_uuid":"eef89c28-3d51-4f46-9d8b-fe6bb40c1e41","_cell_guid":"bd7129de-cba3-46c9-a6ab-64a9c87a2220","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-01T20:13:44.020885Z","iopub.status.idle":"2024-08-01T20:13:44.021291Z","shell.execute_reply.started":"2024-08-01T20:13:44.021106Z","shell.execute_reply":"2024-08-01T20:13:44.021122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate_similarity(sub_cycles, live_sub_cycle, feature, sub_cycle_index, max_comparisons):\n    similarities = []\n\n    #print(\"Calculating Similarities\")\n\n    comparison_count = 0\n    #nominal_cycles = sub_cycles[feature]['nominal'][0][sub_cycle_index]\n    \n    nominal_cycles = sub_cycles[feature]['nominal'][0][sub_cycle_index][::-1]  # Reverse the nominal cycles\n\n    #print(f\"Channel: {feature}, Sub Cycle: {sub_cycle_index}\\n Nominal Cycles:{nominal_cycles}, Live Cycles: {live_sub_cycle}\")\n\n    total_comparisons = len(nominal_cycles) * max_comparisons\n\n    #with tqdm(total=total_comparisons, desc=\"Comparisons\") as pbar:\n    # Compare the live sub-cycle with nominal cycles\n    for nom_cycle in nominal_cycles:\n        if comparison_count >= max_comparisons:\n            break\n        if len(live_sub_cycle) == len(nom_cycle):\n            similarity = irp_utils.compute_similarity(live_sub_cycle, nom_cycle, feature, sub_cycle_index, \"live vs nominal\")\n            similarities.append(similarity)\n            comparison_count += 1\n            #pbar.update(1)\n    #print(comparison_count)\n    return similarities\n\n\nstart_slice = slice_size+(final_sub_cycle_length)*start_index\n\nmax_comparisons = 50#200\nsub_cycle_index=0\nsplit_data = {}\n\n# DataFrame for accumulating data over time\naccumulated_df = pd.DataFrame()\n\n# Initialize the plot\nfig, ax = plt.subplots(figsize=(20, 10))\n\n\nlog_time(\"Live Test Start\")\n\n\n# Loop to simulate the live update\nfor i in range(1000):  # Adjust the range for the number of updates you want\n    start_time = time.time()\n    print(start_slice)\n    # Move slice along\n    start_slice += final_sub_cycle_length# * 6              #if *6, this is 6 sub_cycles\n    end_slice = start_slice + final_sub_cycle_length# * 6   #if *6, this is 6 sub_cycles\n    \n    live_data = full_machine_data.iloc[start_slice:end_slice]\n    live_labels = full_machine_labels.iloc[start_slice:end_slice]\n    \n    j = 0\n\n    # Combine reconstruction errors from all autoencoders\n    reconstruction_errors = []\n    channel_errors = {channel: [] for channel in channels_to_load}  # Dictionary to store errors for each channel\n\n    # DO THING WITH LIVE DATA SLICE HERE\n    for channel_choice in channels_to_load:\n        # For each channel\n\n        print(f\"Comparing channel {channel_choice}, column index {j}\")\n        channel_data = live_data[f'channel_{channel_choice}'].values\n        #print(channel_data.shape)\n\n        if end_slice - start_slice > final_sub_cycle_length:\n            # Split the channel data into sub-cycles\n            sub_cycles_data = np.array_split(channel_data, sub_cycle_divisor)\n            for sub_cycle_index in range(sub_cycle_divisor):\n                \n                sub_cycle_data = sub_cycles_data[sub_cycle_index]\n                \n                similarities = calculate_similarity(sub_cycles, sub_cycle_data, j, sub_cycle_index, max_comparisons)\n                # Convert similarities to DataFrame\n                similarities_df = pd.DataFrame(similarities).fillna(0)\n                #similarities_df = similarities_df.fillna(0)\n                similarities_data = similarities_df.drop(columns=['comparison_type'])#,'rolling_mean_diff','rolling_std_diff','fft_mean_diff','fft_std_diff','fft_max_diff'])\n                #similatities_data = similarities_data.drop(columns=['euclidean_distance','rmse', 'std_diff', 'min_diff', 'max_diff', 'mean_diff', 'fft_diff','var_diff',\n                #'mad_diff','iqr_diff','mad_mean_diff','energy_diff','skew_diff','kurtosis_diff','spectral_entropy_diff'])\n                #similarities_data = similarities_df\n                # Scale the data for autoencoder\n                data_for_autoencoder = scalers[j].transform(similarities_data)\n                # Autoencoder predictions and reconstruction error\n                test_predictions = autoencoders[j].predict(data_for_autoencoder)\n                reconstruction_error = np.mean(np.square(data_for_autoencoder - test_predictions), axis=1)\n                reconstruction_errors.append(reconstruction_error)\n                channel_errors[channel_choice].append(np.mean(reconstruction_error))\n\n\n    \n        else:\n            similarities = calculate_similarity(sub_cycles, channel_data, j, sub_cycle_index, max_comparisons)\n            # Convert similarities to DataFrame\n            similarities_df = pd.DataFrame(similarities).fillna(0)\n            #similarities_df = similarities_df.fillna(0)\n            similarities_data = similarities_df.drop(columns=['comparison_type'])#,'rolling_mean_diff','rolling_std_diff','fft_mean_diff','fft_std_diff','fft_max_diff'])\n            #similatities_data = similarities_data.drop(columns=['euclidean_distance','rmse', 'std_diff', 'min_diff', 'max_diff', 'mean_diff', 'fft_diff','var_diff',\n            #'mad_diff','iqr_diff','mad_mean_diff','energy_diff','skew_diff','kurtosis_diff','spectral_entropy_diff'])\n            #similarities_data = similarities_df\n            # Scale the data for autoencoder\n            data_for_autoencoder = scalers[j].transform(similarities_data)\n            # Autoencoder predictions and reconstruction error\n            test_predictions = autoencoders[j].predict(data_for_autoencoder)\n            reconstruction_error = np.mean(np.square(data_for_autoencoder - test_predictions), axis=1)\n            reconstruction_errors.append(reconstruction_error)\n            channel_errors[channel_choice].append(np.mean(reconstruction_error))\n\n            # Check if the anomaly percentage is under 50%\n            anomaly_percentage = np.mean(reconstruction_errors[-1] > threshold) * 100\n            if anomaly_percentage < 50:\n                print(\"Appending nominal data\")\n                sub_cycles[j]['nominal'].append(channel_data)\n\n\n            sub_cycle_index += 1\n            if sub_cycle_index >= sub_cycle_divisor:\n                sub_cycle_index = 0\n\n        j += 1\n    # Truncate reconstruction errors to the length of the shortest array\n    min_length = min(len(err) for err in reconstruction_errors)\n    truncated_reconstruction_errors = [err[:min_length] for err in reconstruction_errors]\n\n    # Average reconstruction errors\n    avg_reconstruction_error = np.mean(np.vstack(truncated_reconstruction_errors), axis=0)\n\n    # Identify anomalies\n    anomalies = avg_reconstruction_error > threshold\n    print(anomalies)\n    \n        \n#     # Reset sub_cycle_index after processing all channels\n#     if sub_cycle_index >= 6:\n#         sub_cycle_index = 0\n        \n    \n    # Calculate the percentage of anomalies\n    anomaly_percentage = np.mean(anomalies) * 100\n    print(f\"Anomaly percentage: {anomaly_percentage:.2f}%\")\n    #anomaly_percentages.append(anomaly_percentage)\n    \n    # Append current data to accumulated DataFrame\n    live_data = live_data.copy()\n    live_data['labels'] = live_labels['labels']\n    live_data['anomaly_percentage'] = anomaly_percentage\n    \n    # Add channel error ratios and percentages to DataFrame\n    total_error = sum(channel_errors[channel_choice][-1] for channel_choice in channels_to_load)\n    for channel_choice in channels_to_load:\n        if total_error > 0:\n            live_data[f'error_ratio_{channel_choice}'] = channel_errors[channel_choice][-1] / total_error\n            \n        else:\n            live_data[f'error_ratio_{channel_choice}'] = 0\n            \n        live_data[f'error_{channel_choice}'] = channel_errors[channel_choice][-1] if channel_errors[channel_choice] else np.nan\n\n\n\n    accumulated_df = pd.concat([accumulated_df, live_data])\n    \n        # Keep only the latest 12 loops of data\n    if len(accumulated_df) > final_sub_cycle_length * 12:\n        accumulated_df = accumulated_df.iloc[-final_sub_cycle_length * 12:]\n        \n    # Record end time and calculate duration\n    end_time = time.time()\n    evaluation_duration = end_time - start_time\n    print(f\"Evaluation time: {evaluation_duration}\")\n    \n    if i == 1:\n        log_time(\"Live Test Prediction\")\n        # Log the table to wandb\n        wandb.log({\"Time Markers\": time_table})\n\n\n###################################################\n    # Clear and update the plot\n    ax.clear()\n    \n    # Plot anomalies\n    anomaly_indices = accumulated_df[accumulated_df['labels'] == 1].index\n    for anomaly_index in anomaly_indices:\n        ax.axvline(x=anomaly_index, color='red', alpha=0.3, label=\"Ground Truth Anomaly\")\n\n    # Plot the accumulated data\n    colors = plt.colormaps.get_cmap(\"tab10\")  # Get colors for the channels\n    for idx, channel_choice in enumerate(channels_to_load):\n        ax.plot(accumulated_df.index, accumulated_df[f'channel_{channel_choice}'], label=f'Channel {channel_choice}', linestyle='-', color=colors(idx))\n\n    # Add vertical blue lines every final_sub_cycle_length and annotate\n    num_sections = len(accumulated_df) // final_sub_cycle_length\n    for j in range(num_sections + 1):\n        if j * final_sub_cycle_length < len(accumulated_df):\n            ax.axvline(x=accumulated_df.index[j * final_sub_cycle_length], color='blue', linestyle='--', alpha=0.7)\n\n    # Annotate with anomaly percentages along the top x-axis\n    #ylim = [0, 1]\n    #ax.set_ylim(ylim)\n\n    for j in range(num_sections):\n        start_idx = j * final_sub_cycle_length\n        end_idx = min(start_idx + final_sub_cycle_length, len(accumulated_df) - 1)\n        mid_point = accumulated_df.index[start_idx + (end_idx - start_idx) // 2]\n        anomaly_percentage = accumulated_df.iloc[start_idx:end_idx]['anomaly_percentage'].mean()\n        #ax.text(mid_point, ylim[1] + (ylim[1] * 0.05), f'{anomaly_percentage:.2f}%', color='black', fontsize=12, verticalalignment='bottom', horizontalalignment='center')\n        \n        # Convert data coordinates to normalized axes coordinates for annotation\n        x_coord = (mid_point - accumulated_df.index[0]) / (accumulated_df.index[-1] - accumulated_df.index[0])\n        ax.annotate(f'{anomaly_percentage:.2f}%', xy=(x_coord, 1.05), xycoords='axes fraction', color='black', fontsize=12, verticalalignment='bottom', horizontalalignment='center')\n    \n       # Annotate with channel error ratios and percentages for the section\n        for idx, channel_choice in enumerate(channels_to_load):\n            channel_error_ratio = accumulated_df.iloc[start_idx:end_idx][f'error_ratio_{channel_choice}'].mean()\n            #channel_error_percentage = accumulated_df.iloc[start_idx:end_idx][f'error_percentage_{channel_choice}'].mean()\n            #channel_error_percentage = accumulated_df.iloc[start_idx:end_idx][f'error_{channel_choice}'].mean() * 100\n\n            #ax.text(mid_point, ylim[1] + (ylim[1] * 0.1 + idx * 0.05), f'{channel_choice}: {channel_error_ratio:.2f}', color=colors(idx), fontsize=10, verticalalignment='bottom', horizontalalignment='center')\n            y_coord = 1.10 + idx * 0.05\n            ax.annotate(f'{channel_choice}: {channel_error_ratio:.2f}', xy=(x_coord, y_coord), xycoords='axes fraction', color=colors(idx), fontsize=10, verticalalignment='bottom', horizontalalignment='center')\n\n\n    #ax.set_ylim(ylim)  # Reset y-axis limits to avoid adjusting them for text        \n    # Handle legend labels (to avoid duplicate labels in the legend)\n    handles, labels = ax.get_legend_handles_labels()\n    by_label = dict(zip(labels, handles))\n    ax.legend(by_label.values(), by_label.keys(), loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=5)\n\n    ax.set_title('Measurements Over Time with Anomalies for Multiple Channels')\n    ax.set_xlabel('Datetime')\n    ax.set_ylabel('Measurement Value')\n    ax.grid(True)\n    plt.setp(ax.get_xticklabels(), rotation=45)\n    fig.tight_layout()\n\n    # Display the plot\n    clear_output(wait=True)\n    display(fig)\n\n# Show final plot\nplt.show()\n","metadata":{"_uuid":"83323d49-6c29-4a86-bc84-660ae3fc519c","_cell_guid":"03f6f180-8dac-4443-9240-5e0238b74a56","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-08-01T20:13:44.023625Z","iopub.status.idle":"2024-08-01T20:13:44.024033Z","shell.execute_reply.started":"2024-08-01T20:13:44.023818Z","shell.execute_reply":"2024-08-01T20:13:44.023834Z"},"trusted":true},"execution_count":null,"outputs":[]}]}