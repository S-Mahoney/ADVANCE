{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"script","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% [code]\n# %% [markdown]\n# # Utility Scripts for IRP\n\n# %% [code]\n# Loader imports\nimport numpy as np\nfrom scipy.fft import fft, fftfreq\nfrom scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\n\nfrom scipy.spatial.distance import euclidean, cosine\nfrom scipy.stats import pearsonr, spearmanr, skew, kurtosis, entropy\nfrom sklearn.metrics import mean_squared_error\n#from fastdtw import fastdtw\nimport warnings\nimport pywt\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nfrom statsmodels.tsa.ar_model import AutoReg\n\n# Prediction imports\nimport os\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense, Dropout,BatchNormalization,Lambda,InputLayer, GaussianNoise, add\nfrom tensorflow.keras.losses import MeanSquaredError\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.regularizers import l1, l2, l1_l2\nfrom tqdm.notebook import tqdm\nimport re\n\nfrom tensorflow.keras.callbacks import EarlyStopping\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay,f1_score, make_scorer,precision_recall_fscore_support\nfrom sklearn.impute import SimpleImputer\nimport time\nimport random\nimport gc\nimport json\nimport ipywidgets as widgets\nfrom IPython.display import display, clear_output\n\n# %% [markdown]\n# ## Loader functions\n\n# %% [code]\n\n\ndef sine_wave(x, A, B, C, D):\n    return A * np.sin(B * x + C) + D\n\ndef get_subcycle(feature, sub_cycle_divisor, machine_data, machine_labels, \n                 max_freq=1.0, initial_step=0.01, refinement_steps=3, \n                 max_cycle_length=1500, min_cycle_length=400, plot_results=True, \n                 user_cycle_length=None, use_individual_cycle_lengths=False):\n    \"\"\"\n    Function to sort subcycles into arrays of size sub_cycle_divisor of nominal and anomalous data.\n    Input: Chosen feature channel (int), Sub-cycle divisor size (int), Data, Data labels.\n    Output: Nominal subcycles, Anomalous subcycles.\n    \"\"\"\n    print(f\"Feature {feature}\")\n\n    def estimate_cycle_length(y_data, step, already_checked_freqs):\n        offset_estimate = y_data.mean()\n        yf = fft(y_data - offset_estimate)\n        xf = fftfreq(len(y_data), 1)\n        half_len = len(yf) // 2\n        yf = yf[:half_len]\n        xf = xf[:half_len]\n        valid_range = xf <= max_freq\n        yf = yf[valid_range]\n        xf = xf[valid_range]\n        mask = np.isin(xf, already_checked_freqs, invert=True)\n        yf = yf[mask]\n        xf = xf[mask]\n        if len(xf) == 0:\n            return None, None\n        coarse_index = np.arange(0, len(xf), max(1, int(step)))\n        dominant_freq_index = coarse_index[np.argmax(2.0 / len(y_data) * np.abs(yf[coarse_index]))]\n        return xf[dominant_freq_index], dominant_freq_index\n\n    def find_cycle_length(y_data):\n        already_checked_freqs = []\n        dominant_freq, _ = estimate_cycle_length(y_data, initial_step, already_checked_freqs)\n        if dominant_freq:\n            already_checked_freqs.append(dominant_freq)\n        \n        for _ in range(refinement_steps):\n            initial_step = max(1, initial_step // 2)\n            new_freq, _ = estimate_cycle_length(y_data, initial_step, already_checked_freqs)\n            if new_freq:\n                dominant_freq = new_freq\n                already_checked_freqs.append(dominant_freq)\n\n        cycle_length = int(1 / dominant_freq) if dominant_freq else max_cycle_length\n\n        while (cycle_length < min_cycle_length or cycle_length > max_cycle_length) and dominant_freq:\n            if cycle_length < min_cycle_length:\n                new_freq, _ = estimate_cycle_length(y_data, initial_step, already_checked_freqs)\n                if new_freq:\n                    dominant_freq = new_freq\n                    cycle_length = int(1 / dominant_freq)\n                    already_checked_freqs.append(dominant_freq)\n            elif cycle_length > max_cycle_length:\n                new_freq, _ = estimate_cycle_length(y_data, initial_step, already_checked_freqs)\n                if new_freq:\n                    dominant_freq = new_freq\n                    cycle_length = int(1 / dominant_freq)\n                    already_checked_freqs.append(dominant_freq)\n            else:\n                break\n        \n        return cycle_length\n\n    x_data = np.arange(len(machine_data))\n    y_data = machine_data.iloc[:, feature].to_numpy()\n\n    if user_cycle_length:\n        cycle_length = min(max(min_cycle_length, user_cycle_length), max_cycle_length)\n    elif use_individual_cycle_lengths:\n        cycle_length = find_cycle_length(y_data)\n    else:\n        already_checked_freqs = []\n        dominant_freq, _ = estimate_cycle_length(y_data, initial_step, already_checked_freqs)\n        if dominant_freq:\n            already_checked_freqs.append(dominant_freq)\n\n        for _ in range(refinement_steps):\n            initial_step = max(1, initial_step // 2)\n            new_freq, _ = estimate_cycle_length(y_data, initial_step, already_checked_freqs)\n            if new_freq:\n                dominant_freq = new_freq\n                already_checked_freqs.append(dominant_freq)\n\n        cycle_length = int(1 / dominant_freq) if dominant_freq else max_cycle_length\n        print(cycle_length)\n        while (cycle_length < min_cycle_length or cycle_length > max_cycle_length) and dominant_freq:\n            if cycle_length < min_cycle_length:\n                new_freq, _ = estimate_cycle_length(y_data, initial_step, already_checked_freqs)\n                if new_freq:\n                    dominant_freq = new_freq\n                    cycle_length = int(1 / dominant_freq)\n                    already_checked_freqs.append(dominant_freq)\n            elif cycle_length > max_cycle_length:\n                new_freq, _ = estimate_cycle_length(y_data, initial_step, already_checked_freqs)\n                if new_freq:\n                    dominant_freq = new_freq\n                    cycle_length = int(1 / dominant_freq)\n                    already_checked_freqs.append(dominant_freq)\n            else:\n                break\n\n    sub_cycle_length = max(1, int(cycle_length / sub_cycle_divisor))\n\n    print(f\"Cycle Length: {cycle_length}\")\n    print(f\"Sub-Cycle Length: {sub_cycle_length}\")\n\n    if plot_results and cycle_length >= min_cycle_length and cycle_length <= max_cycle_length:\n        plt.figure(figsize=(10, 6))\n        plt.plot(x_data, y_data, label='Data')\n        if not user_cycle_length and dominant_freq:\n            initial_guess = [(y_data.max() - y_data.min()) / 2, 2 * np.pi * dominant_freq, 0, y_data.mean()]\n            try:\n                params, _ = curve_fit(sine_wave, x_data, y_data, p0=initial_guess, maxfev=2000)\n                plt.plot(x_data, sine_wave(x_data, *params), color='lime', label='Fitted Sine Wave')\n            except RuntimeError as e:\n                print(f\"Curve fitting failed: {e}\")\n        for i in np.arange(0, len(x_data), cycle_length):\n            plt.axvline(x=i, color='black', linestyle='-', linewidth=1.5)\n        for i in np.arange(0, len(x_data), sub_cycle_length):\n            plt.axvline(x=i, color='black', linestyle='--', linewidth=1)\n        for start, end in zip(machine_labels.index[machine_labels.iloc[:, 0] == 1], machine_labels.index[machine_labels.iloc[:, 0].shift(-1) == 1]):\n            plt.axvspan(start, end, color='red', alpha=0.5, label='Anomaly' if 'Anomaly' not in plt.gca().get_legend_handles_labels()[1] else \"\")\n        plt.title(f'Sine Wave Fitting to Feature {feature}')\n        plt.xlabel('Time')\n        plt.ylabel('Normalized Value')\n        plt.legend()\n        plt.show()\n\n    nominal_sub_cycles = [[] for _ in range(sub_cycle_divisor)]\n    anomalous_sub_cycles = [[] for _ in range(sub_cycle_divisor)]\n\n    for start in range(0, len(x_data), cycle_length):\n        for i in range(sub_cycle_divisor):\n            sub_cycle_start = start + i * sub_cycle_length\n            sub_cycle_end = sub_cycle_start + sub_cycle_length\n            if sub_cycle_end <= len(x_data):\n                sub_cycle_data = y_data[sub_cycle_start:sub_cycle_end]\n                sub_cycle_labels = machine_labels.iloc[sub_cycle_start:sub_cycle_end, 0].to_numpy()\n                if np.any(sub_cycle_labels == 1):\n                    anomalous_sub_cycles[i].append(sub_cycle_data)\n                else:\n                    nominal_sub_cycles[i].append(sub_cycle_data)\n\n    return nominal_sub_cycles, anomalous_sub_cycles, cycle_length\n\n#Check that we dont enable conflicting options\ndef validate_cycle_length_selection(user_defined_cycle_length, use_average_cycle_length, use_most_common_cycle_length, use_individual_cycle_length):\n    # Create a list of the boolean variables\n    options = [user_defined_cycle_length, use_average_cycle_length, use_most_common_cycle_length, use_individual_cycle_length]\n    \n    # Count the number of False values\n    false_count = options.count(True)\n    \n    # Assert that no more than one option is False\n    assert false_count <= 1, \"More than one option is set to False, which is not allowed.\"\n    \ndef compute_wavelet_features(signal, wavelet='db1', level=4):\n    coeffs = pywt.wavedec(signal, wavelet, level=level)\n    features = []\n    for coeff in coeffs:\n        features.append(np.mean(coeff))\n        features.append(np.std(coeff))\n        features.append(np.sum(coeff**2))  # Energy of coefficients\n    return np.concatenate(features)\n\ndef is_near_constant(array, epsilon=1e-6):\n    \"\"\"Check if an array is near constant\"\"\"\n    return np.ptp(array) < epsilon\n\ndef fourier_transform_features(data):\n    fft_vals = np.fft.fft(data)\n    fft_power = np.abs(fft_vals)\n    return {\n        'fft_mean': np.mean(fft_power),\n        'fft_std': np.std(fft_power),\n        'fft_max': np.max(fft_power),\n    }\n\ndef rolling_features(data, window_size=5):\n    features = pd.DataFrame()\n    features['rolling_mean'] = data.rolling(window=window_size).mean()\n    features['rolling_std'] = data.rolling(window=window_size).std()\n    features['rolling_var'] = data.rolling(window=window_size).var()\n    return features\n\ndef compute_mahalanobis(nom_cycle, anom_cycle):\n    \"\"\"Compute Mahalanobis distance between two cycles\"\"\"\n    data = np.vstack([nom_cycle, anom_cycle])\n    mean = np.mean(data, axis=0)\n    cov = np.cov(data.T)\n    inv_covmat = np.linalg.inv(cov)\n    return mahalanobis(nom_cycle, anom_cycle, inv_covmat)\n\ndef ar_model_coefficients(data, lags=5):\n    model = AutoReg(data, lags=lags).fit()\n    return model.params\n\ndef compute_spectral_entropy(signal):\n    \"\"\"Compute spectral entropy of a signal\"\"\"\n    if len(signal) < 2:\n        return 0\n    psd = np.abs(np.fft.fft(signal))**2\n    psd_sum = psd.sum()\n    if psd_sum == 0:\n        return 0\n    psd_norm = psd / psd_sum\n    return entropy(psd_norm)\n\ndef compute_similarity(cycle_1, cycle_2, feature, sub_cycle_index, comparison_type):\n    # Check for near constant arrays\n    cycle1_near_constant = is_near_constant(cycle_1)\n    cycle2_near_constant = is_near_constant(cycle_2)\n    \n    # Calculate Pearson and Spearman correlations\n    if cycle1_near_constant or cycle2_near_constant:\n        pearson_corr = 0\n        spearman_corr = 0\n    else:\n        try:\n            pearson_corr, _ = pearsonr(cycle_1, cycle_2)\n        except Exception as e:\n            print(f\"Pearson calculation error for feature {feature}, sub-cycle {sub_cycle_index + 1}: {e}\")\n            pearson_corr = 0\n        \n        try:\n            spearman_corr, _ = spearmanr(cycle_1, cycle_2)\n        except Exception as e:\n            print(f\"Spearman calculation error for feature {feature}, sub-cycle {sub_cycle_index + 1}: {e}\")\n            spearman_corr = 0\n    \n    # Calculate Euclidean distance\n    euclidean_dist = euclidean(cycle_1, cycle_2)\n    \n    # Calculate Cosine similarity\n    if np.all(cycle_1 == 0) or np.all(cycle_2 == 0):\n        cosine_sim = 0\n    else:\n        cosine_sim = 1 - cosine(cycle_1, cycle_2)\n\n    # Calculate RMSE\n    rmse_val = np.sqrt(mean_squared_error(cycle_1, cycle_2))\n        \n#     # Calculate DTW distance\n#     try:\n#         dtw_dist = compute_dtw(cycle_1, cycle_2)\n#     except:\n#         dtw_dist = np.nan\n\n    if cycle1_near_constant or cycle2_near_constant:\n        skew_diff = 0\n        kurtosis_diff = 0\n    elif len(cycle_1) > 1 and len(cycle_2) > 1:\n        try:\n            skew_diff = np.abs(skew(cycle_1) - skew(cycle_2))\n        except:\n            skew_diff = 0\n        try:\n            kurtosis_diff = np.abs(kurtosis(cycle_1) - kurtosis(cycle_2))\n        except:\n            kurtosis_diff = 0\n    else:\n        skew_diff = 0\n        kurtosis_diff = 0\n        \n    # Calculate Spectral Entropy\n    try:\n        spectral_entropy_diff = np.abs(compute_spectral_entropy(cycle_1) - compute_spectral_entropy(cycle_2))\n    except Exception as e:\n        print(f\"Spectral Entropy calculation error for feature {feature}, sub-cycle {sub_cycle_index + 1}: {e}\")\n        spectral_entropy_diff = 0\n    \n    # Calculate additional statistics\n    std_diff = np.abs(np.std(cycle_1) - np.std(cycle_2))\n    min_diff = np.abs(np.min(cycle_1) - np.min(cycle_2))\n    max_diff = np.abs(np.max(cycle_1) - np.max(cycle_2))\n    mean_diff = np.abs(np.mean(cycle_1) - np.mean(cycle_2))\n    var_diff = np.abs(np.var(cycle_1) - np.var(cycle_2))\n    mad_diff = np.abs(np.median(np.abs(cycle_1 - np.median(cycle_1))) - \n                      np.median(np.abs(cycle_2 - np.median(cycle_2))))\n    iqr_diff = np.abs(np.percentile(cycle_1, 75) - np.percentile(cycle_1, 25) - \n                      np.percentile(cycle_2, 75) + np.percentile(cycle_2, 25))\n    mad_mean_diff = np.abs(np.mean(np.abs(cycle_1 - np.mean(cycle_1))) - \n                           np.mean(np.abs(cycle_2 - np.mean(cycle_2))))\n    energy_diff = np.abs(np.sum(cycle_1**2) - np.sum(cycle_2**2))\n    \n\n    # Calculate FFT and compare the magnitudes\n    if len(cycle_1) > 10 and len(cycle_2) > 10:  # Only calculate FFT if length > 10\n        try:\n            fft_nom = np.abs(np.fft.fft(cycle_1))\n            fft_anom = np.abs(np.fft.fft(cycle_2))\n            fft_diff = np.mean(np.abs(fft_nom - fft_anom))\n        except Exception as e:\n            print(f\"FFT calculation error for feature {feature}, sub-cycle {sub_cycle_index + 1}: {e}\")\n            fft_diff = np.nan\n    else:\n        fft_diff = np.nan\n\n#     try:\n#         wavelet_cycle1 = compute_wavelet_features(cycle_1)\n#         wavelet_cycle2 = compute_wavelet_features(cycle_2)\n#         wavelet_diff = np.mean(np.abs(wavelet_cycle1 - wavelet_cycle2))\n#     except Exception as e:\n#         wavelet_diff = np.nan\n\n    # Calculate Rolling Statistics\n    rolling_1 = rolling_features(pd.Series(cycle_1)).dropna()\n    rolling_2 = rolling_features(pd.Series(cycle_2)).dropna()\n    \n    # Calculate Fourier Transform Features\n    fft_features_1 = fourier_transform_features(cycle_1)\n    fft_features_2 = fourier_transform_features(cycle_2)\n    \n    # Calculate AR Model Coefficients\n    #ar_coeffs_1 = ar_model_coefficients(cycle_1)\n    #ar_coeffs_2 = ar_model_coefficients(cycle_2)\n\n    return {\n        'feature': feature,\n        'sub_cycle_index': sub_cycle_index + 1,\n        'comparison_type': comparison_type,\n        'pearson_correlation': pearson_corr,\n        'spearman_correlation': spearman_corr,\n        'euclidean_distance': euclidean_dist,\n        'cosine_similarity': cosine_sim,\n        'rmse': rmse_val,\n        #'dtw_distance': dtw_dist,\n        'std_diff': std_diff,\n        'min_diff': min_diff,\n        'max_diff': max_diff,\n        'mean_diff': mean_diff,\n        'var_diff': var_diff,\n        'mad_diff': mad_diff,\n        'iqr_diff': iqr_diff,\n        'mad_mean_diff': mad_mean_diff,\n        'energy_diff': energy_diff,\n        'fft_diff': fft_diff,\n        #'wavelet_diff': wavelet_diff,\n        'skew_diff': skew_diff,\n        'kurtosis_diff': kurtosis_diff,\n        'spectral_entropy_diff': spectral_entropy_diff,\n        'rolling_mean_diff': np.abs(rolling_1['rolling_mean'].mean() - rolling_2['rolling_mean'].mean()),\n        'rolling_std_diff': np.abs(rolling_1['rolling_std'].mean() - rolling_2['rolling_std'].mean()),\n        'fft_mean_diff': np.abs(fft_features_1['fft_mean'] - fft_features_2['fft_mean']),\n        'fft_std_diff': np.abs(fft_features_1['fft_std'] - fft_features_2['fft_std']),\n        'fft_max_diff': np.abs(fft_features_1['fft_max'] - fft_features_2['fft_max']),\n        #'ar_coeff_diff': np.linalg.norm(ar_coeffs_1 - ar_coeffs_2)\n    }\n\n\n\ndef calculate_similarity(sub_cycles, max_nominal_comparisons, max_anomalous_comparisons,sub_cycle_divisor):\n    nominal_similarities = []\n    anomalous_similarities = []\n\n    print(\"Calculating Similarities\")\n\n    total_nominal_comparisons = len(sub_cycles.keys()) * sub_cycle_divisor * max_nominal_comparisons\n    total_anomalous_comparisons = len(sub_cycles.keys()) * sub_cycle_divisor * max_anomalous_comparisons\n\n    with tqdm(total=total_nominal_comparisons, desc=\"Nominal Comparisons\") as nominal_pbar, \\\n         tqdm(total=total_anomalous_comparisons, desc=\"Anomalous Comparisons\") as anomalous_pbar:\n        for feature in sub_cycles.keys():\n            for sub_cycle_index in range(sub_cycle_divisor):  # Assuming 3 sub-cycles\n                nominal_comparison_count = 0\n                anomalous_comparison_count = 0\n\n                nominal_cycles = sub_cycles[feature]['nominal'][0][sub_cycle_index]\n                anomalous_cycles = sub_cycles[feature].get('anomalous', [[], [], []])[0][sub_cycle_index]\n\n                # Compare nominal cycles\n                for j in range(len(nominal_cycles)):\n                    if nominal_comparison_count >= max_nominal_comparisons:\n                        break\n                    for k in range(j + 1, len(nominal_cycles)):\n                        if nominal_comparison_count >= max_nominal_comparisons:\n                            break\n                        if len(nominal_cycles[j]) == len(nominal_cycles[k]):\n                            similarity = compute_similarity(nominal_cycles[j], nominal_cycles[k], feature, sub_cycle_index, \"nominal\")\n                            nominal_similarities.append(similarity)\n                            nominal_comparison_count += 1\n                            nominal_pbar.update(1)\n\n                # Compare nominal and anomalous cycles if anomalous cycles exist\n                for nom_cycle in nominal_cycles:\n                    if anomalous_comparison_count >= max_anomalous_comparisons:\n                        break\n                    for anom_cycle in anomalous_cycles:\n                        if anomalous_comparison_count >= max_anomalous_comparisons:\n                            break\n                        if len(nom_cycle) == len(anom_cycle):\n                            similarity = compute_similarity(nom_cycle, anom_cycle, feature, sub_cycle_index, \"anomalous\")\n                            anomalous_similarities.append(similarity)\n                            anomalous_comparison_count += 1\n                            anomalous_pbar.update(1)\n\n    return nominal_similarities, anomalous_similarities\n\ndef plot_features(nominal_df, anomalous_df, sub_cycle_divisor):\n    features = [col for col in nominal_df.columns if col not in ['feature', 'sub_cycle_index', 'comparison_type']]\n    sub_cycle_indices = nominal_df['sub_cycle_index'].unique()\n    \n    for feature in features:\n        fig, axs = plt.subplots(1, sub_cycle_divisor, figsize=(sub_cycle_divisor * 5, 5), sharey=True)\n        \n        for i, sub_cycle_index in enumerate(sub_cycle_indices):\n            nominal_values = nominal_df[nominal_df['sub_cycle_index'] == sub_cycle_index][feature]\n            anomalous_values = anomalous_df[anomalous_df['sub_cycle_index'] == sub_cycle_index][feature]\n            \n            x_labels = ['Nominal', 'Anomalous']\n            x_ticks = [0, 1]\n            \n            # Plot nominal data\n            axs[i].scatter([x_ticks[0]] * len(nominal_values), nominal_values, color='blue', label='Nominal' if i == 0 else \"\")\n            \n            # Plot anomalous data\n            axs[i].scatter([x_ticks[1]] * len(anomalous_values), anomalous_values, color='red', label='Anomalous' if i == 0 else \"\")\n            \n            axs[i].set_xticks(x_ticks)\n            axs[i].set_xticklabels(x_labels)\n            axs[i].set_title(f'Sub Cycle Index: {sub_cycle_index}')\n            axs[i].set_ylabel(feature)\n            axs[i].grid(True)\n        \n        handles, labels = axs[0].get_legend_handles_labels()\n        fig.legend(handles, labels, loc='upper right')\n        fig.suptitle(f'Feature: {feature}', y=1.02, fontsize=16)\n        plt.tight_layout()\n        plt.show()\n\n# %% [markdown]\n# ## Prediction functions\n\n# %% [code]\n# Define the f1c_score function\ndef f1c_score(y_true, y_pred):\n    def get_events(y):\n        events = []\n        in_event = False\n        start = 0\n        for i in range(len(y)):\n            if y[i] == 1 and not in_event:\n                in_event = True\n                start = i\n            elif y[i] == 0 and in_event:\n                in_event = False\n                events.append((start, i))\n        if in_event:\n            events.append((start, len(y)))\n        return events\n\n    y_true = np.array(y_true)  # Ensure y_true is a numpy array\n    y_pred = np.array(y_pred)  # Ensure y_pred is a numpy array\n\n    true_events = get_events(y_true)\n    pred_events = get_events(y_pred)\n\n    tp_events = sum(any(pred_start <= true_start <= pred_end or pred_start <= true_end <= pred_end\n                        for pred_start, pred_end in pred_events) for true_start, true_end in true_events)\n    fp_events = sum(not any(pred_start <= true_start <= pred_end or pred_start <= true_end <= pred_end\n                            for true_start, true_end in true_events) for pred_start, pred_end in pred_events)\n    fn_events = len(true_events) - tp_events\n\n    precision_event = tp_events / (tp_events + fp_events) if tp_events + fp_events > 0 else 0\n    recall_event = tp_events / (tp_events + fn_events) if tp_events + fn_events > 0 else 0\n    f1c = 2 * (precision_event * recall_event) / (precision_event + recall_event) if precision_event + recall_event > 0 else 0\n    return f1c\n\n# Custom evaluation function for XGBoost\ndef custom_f1c_eval(y_pred, dtrain):\n    y_true = dtrain.get_label()\n    y_pred_binary = np.round(y_pred)  # Convert probabilities to binary predictions\n    return 'f1c', f1c_score(y_true, y_pred_binary)\n\n# Function to introduce random NaNs in the data\ndef introduce_nans(data, nan_fraction=0.05, random_state=42):\n    np.random.seed(random_state)\n    nan_mask = np.random.rand(*data.shape) < nan_fraction\n    data[nan_mask] = np.nan\n    return data\n\n# Function to save the uploaded file\ndef handle_upload(change):\n    global uploaded_file_path\n    # Get the uploaded file's metadata\n    uploaded_filename = list(upload_button.value.keys())[0]\n    uploaded_file = upload_button.value[uploaded_filename]\n\n    # Save the file to the Kaggle working directory\n    uploaded_file_path = os.path.join('/kaggle/working', uploaded_filename)\n    with open(uploaded_file_path, 'wb') as f:\n        f.write(uploaded_file['content'])\n\n    print(f\"File uploaded and saved to {uploaded_file_path}\")\n    clear_output()\n    display(upload_button)\n    display(load_button)\n\n# Function to load the dictionary from JSON\ndef load_from_json():\n    global loaded_config, uploaded_file_path\n    if uploaded_file_path:\n        with open(uploaded_file_path, 'r') as json_file:\n            autoencoders_dict = json.load(json_file)\n        for key, value in autoencoders_dict.items():\n            for func in value['functions']:\n                func['regularization'] = func['regularization']\n        loaded_config = autoencoders_dict\n        print(\"Loaded configuration:\", loaded_config)\n    else:\n        print(\"No file has been uploaded yet.\")\n        \n# Function to train individual autoencoder for a single feature\ndef train_autoencoder(X_train, X_val, autoencoders_dict, index):\n    # Build the autoencoder model\n    input_dim = X_train.shape[1]\n    encoding_dim = X_train.shape[1]  # Set encoding dimension to a lower value\n\n    input_layer = Input(shape=(input_dim,))\n    x = GaussianNoise(0.1)(input_layer)\n\n    # Encoder\n    x = Dense(64, activation='relu', activity_regularizer=l2(0.01))(x)\n    x = BatchNormalization()(x)\n    x = Dense(32, activation='relu', activity_regularizer=l2(0.01))(x)\n    x = BatchNormalization()(x)\n    x = Dense(encoding_dim, activation='relu', activity_regularizer=l2(0.01))(x)\n    x = Dropout(0.2)(x)\n\n    encoder_output = add([x, input_layer])\n\n    # Decoder\n    x = Dense(32, activation='relu', activity_regularizer=l2(0.01))(encoder_output)\n    x = BatchNormalization()(x)\n    x = Dense(64, activation='relu', activity_regularizer=l2(0.01))(x)\n    x = BatchNormalization()(x)\n    x = Dense(input_dim, activation='relu', activity_regularizer=l2(0.01))(x)\n\n    autoencoder = Model(inputs=input_layer, outputs=x)\n    autoencoder.compile(optimizer='adam', loss='mse')\n\n    # Early stopping to prevent overfitting\n    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n\n    # Train the autoencoder\n    history = autoencoder.fit(X_train, X_train,\n                              epochs=30,\n                              batch_size=128,\n                              shuffle=True,\n                              validation_data=(X_val, X_val),\n                              callbacks=[early_stopping],\n                              verbose=0)\n\n    # Storing the model in the dictionary\n    autoencoders_dict[f'autoencoder_{index}'] = {'model': autoencoder}\n\n    return autoencoder, X_val, history\n\n# Define the f1pa_score function\ndef f1pa_score(y_true, y_pred):\n    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n    return precision, recall, f1\n\n# Split the nominal data into training, validation, and test sets before training\ndef split_nominal_data(nominal_df, test_size=0.4, val_size=0.5, random_state=42):\n    \n    random_state = random.randint(0, 1000)\n    #print(f\"random_state: {random_state}\")\n    X_train, X_temp = train_test_split(nominal_df, test_size=test_size, random_state=random_state)\n    X_val, X_test_nominal = train_test_split(X_temp, test_size=val_size, random_state=random_state)\n    return X_train, X_val, X_test_nominal\n\n\n# Function to evaluate the ensemble of autoencoders\ndef evaluate_ensemble(autoencoders, X_test_combined, y_test, val_data_list, percentile_threshold ,global_encoder=None):\n    # Combine reconstruction errors from all autoencoders\n    reconstruction_errors = []\n    for autoencoder, feature_data in zip(autoencoders, X_test_combined):\n        #feature_data = imputer.transform(feature_data)  # Impute missing values\n        test_predictions = autoencoder.predict(feature_data)\n        reconstruction_error = np.mean(np.square(feature_data - test_predictions), axis=1)\n        reconstruction_errors.append(reconstruction_error)\n        \n    # Average reconstruction errors\n    avg_reconstruction_error = np.mean(np.vstack(reconstruction_errors), axis=0)\n\n    # Set a threshold for anomaly detection\n    #This section of code is a bit shitty but seems to do the job. Takes X_val from val_data_list, which is created in the next code block\n    val_reconstruction_errors = [np.mean(np.square(X_val - autoencoder.predict(X_val)), axis=1) for autoencoder, X_val in zip(autoencoders, val_data_list)]\n    combined_val_errors = np.mean(np.vstack(val_reconstruction_errors), axis=0)\n    threshold = np.percentile(combined_val_errors, percentile_threshold)\n    \n    # Identify anomalies\n    anomalies = avg_reconstruction_error > threshold\n\n    # Compare predictions with actual labels\n    f1c = f1c_score(y_test, anomalies.astype(int))\n    f1 = f1_score(y_test, anomalies.astype(int))\n    precision, recall, f1pa = f1pa_score(y_test, anomalies.astype(int))\n\n    # Create a DataFrame for better visualization\n    results_df = pd.DataFrame({\n        'Reconstruction Error': avg_reconstruction_error,\n        'Actual Label': y_test,\n        'Predicted Anomaly': anomalies.astype(int),\n        'Correct Prediction': (anomalies == y_test)\n    })\n\n\n    return results_df, f1c, f1, precision, recall, f1pa, threshold\n    \ndef compute_gradient_descent(history):\n    losses = history.history['val_loss']\n    gradients = np.diff(losses)\n    return np.min(gradients)","metadata":{"_uuid":"e55d16e5-54eb-4189-8096-384b828652fd","_cell_guid":"18c72c73-fbef-493e-9fcb-359a117f7797","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}